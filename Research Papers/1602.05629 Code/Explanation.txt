Model 1:
A simple multilayer-perceptron with 2-hidden layers with 200 units each using ReLu activations (199,210 total parameters), which we refer to as the MNIST 2NN.

			Activation Shape	Activation Size		#Parameters
Input		(28, 28, 1)			784					0
FC1			(200, 1)			200					(28*28+1)*200=157000
FC2			(200, 1)			200					(200*1+1)*200=40200
Softmax		(10, 1)				10					(200*1+1)*10=2010

157,000 + 40,200 + 2,010 = 199,210


Model 2:
A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters).

With Padding
				Activation Shape	Activation Size		#Parameters
Input			(28, 28, 1)			784					0
CONV1(f=5, s=1)	(28, 28, 32)		25088				(5*5*1+1)*32=832
POOL1			(14, 14, 32)		6272				0
CONV2(f=5, s=1)	(14, 14, 64)		12544				(5*5*32+1)*64=51264
POOL2			(7, 7, 64)			3136				0
FC3				(512, 1)			512					(7*7*64+1)*512=1606144
Softmax			(10, 1)				10					(512*1+1)*10=5130

832 + 51,264 + 1,606,144 + 5,130 = 1,663,370


Without Padding
				Activation Shape	Activation Size		#Parameters
Input			(28, 28, 1)			784					0
CONV1(f=5, s=1)	(24, 24, 32)		18432				(5*5*1+1)*32=832
POOL1			(12, 12, 32)		4602				0
CONV2(f=5, s=1)	(8, 8, 64)			256					(5*5*32+1)*64=51264
POOL2			(4, 4, 64)			512					0
FC3				(512, 1)			512					(4*4*64+1)*512=262656
Softmax			(10, 1)				10					(512*1+1)*10=5130

832 + 51,264 + 262,656 + 5,130 = 319,882